{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from albumentations import BboxParams\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.python.keras import layers, models, regularizers\n",
    "\n",
    "import fastestimator as fe\n",
    "from fastestimator.backend.to_number import to_number\n",
    "from fastestimator.dataset.data import mscoco\n",
    "from fastestimator.op.numpyop import NumpyOp\n",
    "from fastestimator.op.numpyop.meta import Sometimes\n",
    "from fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded\n",
    "from fastestimator.op.numpyop.univariate import Normalize, ReadImage, ToArray\n",
    "from fastestimator.op.tensorop import TensorOp\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "from fastestimator.trace.adapt import LRScheduler\n",
    "from fastestimator.trace.io import BestModelSaver\n",
    "from fastestimator.trace.metric import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 12 \n",
    "max_steps_per_epoch = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('class.json', 'r') as f:\n",
    "    class_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, eval_ds = mscoco.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_ds) == 118287\n",
    "assert len(eval_ds) == 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_fpn_anchor_box(width: int, height: int):\n",
    "    assert height % 32 == 0 and width % 32 == 0\n",
    "    shapes = [(int(height / 8), int(width / 8))]  # P3\n",
    "    num_pixel = [np.prod(shapes)]\n",
    "    anchor_lengths = [32, 64, 128, 256, 512]\n",
    "    for _ in range(4):  # P4 through P7\n",
    "        shapes.append((int(np.ceil(shapes[-1][0] / 2)), int(np.ceil(shapes[-1][1] / 2))))\n",
    "        num_pixel.append(np.prod(shapes[-1]))\n",
    "    total_num_pixels = np.sum(num_pixel)\n",
    "    anchorbox = np.zeros((9 * total_num_pixels, 4))\n",
    "    anchor_length_multipliers = [2**(0.0), 2**(1 / 3), 2**(2 / 3)]\n",
    "    aspect_ratios = [1.0, 2.0, 0.5]  #x:y\n",
    "    anchor_idx = 0\n",
    "    for shape, anchor_length in zip(shapes, anchor_lengths):\n",
    "        p_h, p_w = shape\n",
    "        base_y = 2**np.ceil(np.log2(height / p_h))\n",
    "        base_x = 2**np.ceil(np.log2(width / p_w))\n",
    "        for i in range(p_h):\n",
    "            center_y = (i + 1 / 2) * base_y\n",
    "            for j in range(p_w):\n",
    "                center_x = (j + 1 / 2) * base_x\n",
    "                for anchor_length_multiplier in anchor_length_multipliers:\n",
    "                    area = (anchor_length * anchor_length_multiplier)**2\n",
    "                    for aspect_ratio in aspect_ratios:\n",
    "                        x1 = center_x - np.sqrt(area * aspect_ratio) / 2\n",
    "                        y1 = center_y - np.sqrt(area / aspect_ratio) / 2\n",
    "                        x2 = center_x + np.sqrt(area * aspect_ratio) / 2\n",
    "                        y2 = center_y + np.sqrt(area / aspect_ratio) / 2\n",
    "                        anchorbox[anchor_idx, 0] = x1\n",
    "                        anchorbox[anchor_idx, 1] = y1\n",
    "                        anchorbox[anchor_idx, 2] = x2 - x1\n",
    "                        anchorbox[anchor_idx, 3] = y2 - y1\n",
    "                        anchor_idx += 1\n",
    "        if p_h == 1 and p_w == 1:  # the next level of 1x1 feature map is still 1x1, therefore ignore\n",
    "            break\n",
    "    return np.float32(anchorbox), np.int32(num_pixel) * 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorBox(NumpyOp):\n",
    "    def __init__(self, width, height, inputs, outputs, mode=None):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.anchorbox, _ = _get_fpn_anchor_box(width, height)  # anchorbox is #num_anchor x 4\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        target = self._generate_target(data)  # bbox is #obj x 5\n",
    "        return np.float32(target)\n",
    "\n",
    "    def _generate_target(self, bbox):\n",
    "        object_boxes = bbox[:, :-1]  # num_obj x 4\n",
    "        label = bbox[:, -1]  # num_obj x 1\n",
    "        ious = self._get_iou(object_boxes, self.anchorbox)  # num_obj x num_anchor\n",
    "        #now for each object in image, assign the anchor box with highest iou to them\n",
    "        anchorbox_best_iou_idx = np.argmax(ious, axis=1)\n",
    "        num_obj = ious.shape[0]\n",
    "        for row in range(num_obj):\n",
    "            ious[row, anchorbox_best_iou_idx[row]] = 0.99\n",
    "        #next, begin the anchor box assignment based on iou\n",
    "        anchor_to_obj_idx = np.argmax(ious, axis=0)  # num_anchor x 1\n",
    "        anchor_best_iou = np.max(ious, axis=0)  # num_anchor x 1\n",
    "        cls_gt = np.int32([label[idx] for idx in anchor_to_obj_idx])  # num_anchor x 1\n",
    "        cls_gt[np.where(anchor_best_iou <= 0.4)] = -1  #background class\n",
    "        cls_gt[np.where(np.logical_and(anchor_best_iou > 0.4, anchor_best_iou <= 0.5))] = -2  # ignore these examples\n",
    "        #finally, calculate localization target\n",
    "        single_loc_gt = object_boxes[anchor_to_obj_idx]  # num_anchor x 4\n",
    "        gt_x1, gt_y1, gt_width, gt_height = np.split(single_loc_gt, 4, axis=1)\n",
    "        ac_x1, ac_y1, ac_width, ac_height = np.split(self.anchorbox, 4, axis=1)\n",
    "        dx1 = np.squeeze((gt_x1 - ac_x1) / ac_width)\n",
    "        dy1 = np.squeeze((gt_y1 - ac_y1) / ac_height)\n",
    "        dwidth = np.squeeze(np.log(gt_width / ac_width))\n",
    "        dheight = np.squeeze(np.log(gt_height / ac_height))\n",
    "        return np.array([dx1, dy1, dwidth, dheight, cls_gt]).T  # num_anchor x 5\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_iou(boxes1, boxes2):\n",
    "        \"\"\"Computes the value of intersection over union (IoU) of two array of boxes.\n",
    "        Args:\n",
    "            box1 (array): first boxes in N x 4\n",
    "            box2 (array): second box in M x 4\n",
    "        Returns:\n",
    "            float: IoU value in N x M\n",
    "        \"\"\"\n",
    "        x11, y11, w1, h1 = np.split(boxes1, 4, axis=1)\n",
    "        x21, y21, w2, h2 = np.split(boxes2, 4, axis=1)\n",
    "        x12 = x11 + w1\n",
    "        y12 = y11 + h1\n",
    "        x22 = x21 + w2\n",
    "        y22 = y21 + h2\n",
    "        xmin = np.maximum(x11, np.transpose(x21))\n",
    "        ymin = np.maximum(y11, np.transpose(y21))\n",
    "        xmax = np.minimum(x12, np.transpose(x22))\n",
    "        ymax = np.minimum(y12, np.transpose(y22))\n",
    "        inter_area = np.maximum((xmax - xmin + 1), 0) * np.maximum((ymax - ymin + 1), 0)\n",
    "        area1 = (w1 + 1) * (h1 + 1)\n",
    "        area2 = (w2 + 1) * (h2 + 1)\n",
    "        iou = inter_area / (area1 + area2.T - inter_area)\n",
    "        return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = fe.Pipeline(\n",
    "    train_data=train_ds,\n",
    "    eval_data=eval_ds.split(0.01),\n",
    "    batch_size=batch_size,\n",
    "    ops=[\n",
    "        ReadImage(inputs=\"image\", outputs=\"image\"),\n",
    "        LongestMaxSize(512,\n",
    "                       image_in=\"image\",\n",
    "                       image_out=\"image\",\n",
    "                       bbox_in=\"bbox\",\n",
    "                       bbox_out=\"bbox\",\n",
    "                       bbox_params=BboxParams(\"coco\", min_area=1.0)),\n",
    "        PadIfNeeded(\n",
    "            512,\n",
    "            512,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            image_in=\"image\",\n",
    "            image_out=\"image\",\n",
    "            bbox_in=\"bbox\",\n",
    "            bbox_out=\"bbox\",\n",
    "            bbox_params=BboxParams(\"coco\", min_area=1.0),\n",
    "        ),\n",
    "        Sometimes(\n",
    "            HorizontalFlip(mode=\"train\",\n",
    "                           image_in=\"image\",\n",
    "                           image_out=\"image\",\n",
    "                           bbox_in=\"bbox\",\n",
    "                           bbox_out=\"bbox\",\n",
    "                           bbox_params='coco')),\n",
    "        Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),\n",
    "        ToArray(inputs=\"bbox\", outputs=\"bbox\", dtype=\"float32\"),\n",
    "        AnchorBox(inputs=\"bbox\", outputs=\"anchorbox\", width=512, height=512)\n",
    "    ],\n",
    "    pad_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = pipeline.get_results(mode='eval', num_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_index = 6\n",
    "# step_index = 1\n",
    "\n",
    "# img = batch_data[step_index]['image'][batch_index].numpy()\n",
    "# img = ((img + 1)/2 * 255).astype(np.uint8)\n",
    "\n",
    "# keep = batch_data[step_index]['bbox'][batch_index].numpy()[..., -1] > 0\n",
    "# x1, y1, w, h, label = batch_data[step_index]['bbox'][batch_index].numpy()[keep].T\n",
    "# x2 = x1 + w\n",
    "# y2 = y1 + h\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# for j in range(len(x1)):\n",
    "#     cv2.rectangle(img, (x1[j], y1[j]), (x2[j], y2[j]), (0, 0, 255), 2)\n",
    "#     ax.text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]))], color=(0, 0, 1), fontsize=14, fontweight='bold')\n",
    "\n",
    "# ax.imshow(img)\n",
    "# print(\"id = {}\".format(batch_data[step_index]['image_id'][batch_index].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _classification_sub_net(num_classes, num_anchor=9):\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(num_classes * num_anchor,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='sigmoid',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer=tf.initializers.constant(np.log(1 / 99))))\n",
    "    model.add(layers.Reshape((-1, num_classes)))  # the output dimension is [batch, #anchor, #classes]\n",
    "    return model\n",
    "\n",
    "\n",
    "def _regression_sub_net(num_anchor=9):\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(4 * num_anchor,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(layers.Reshape((-1, 4)))  # the output dimension is [batch, #anchor, 4]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RetinaNet(input_shape, num_classes, num_anchor=9):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    # FPN\n",
    "    resnet50 = tf.keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_tensor=inputs, pooling=None)\n",
    "    assert resnet50.layers[80].name == \"conv3_block4_out\"\n",
    "    C3 = resnet50.layers[80].output\n",
    "    assert resnet50.layers[142].name == \"conv4_block6_out\"\n",
    "    C4 = resnet50.layers[142].output\n",
    "    assert resnet50.layers[-1].name == \"conv5_block3_out\"\n",
    "    C5 = resnet50.layers[-1].output\n",
    "    P5 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.0001))(C5)\n",
    "    P5_upsampling = layers.UpSampling2D()(P5)\n",
    "    P4 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.0001))(C4)\n",
    "    P4 = layers.Add()([P5_upsampling, P4])\n",
    "    P4_upsampling = layers.UpSampling2D()(P4)\n",
    "    P3 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.0001))(C3)\n",
    "    P3 = layers.Add()([P4_upsampling, P3])\n",
    "    P6 = layers.Conv2D(256,\n",
    "                       kernel_size=3,\n",
    "                       strides=2,\n",
    "                       padding='same',\n",
    "                       name=\"P6\",\n",
    "                       kernel_regularizer=regularizers.l2(0.0001))(C5)\n",
    "    P7 = layers.Activation('relu')(P6)\n",
    "    P7 = layers.Conv2D(256,\n",
    "                       kernel_size=3,\n",
    "                       strides=2,\n",
    "                       padding='same',\n",
    "                       name=\"P7\",\n",
    "                       kernel_regularizer=regularizers.l2(0.0001))(P7)\n",
    "    P5 = layers.Conv2D(256,\n",
    "                       kernel_size=3,\n",
    "                       strides=1,\n",
    "                       padding='same',\n",
    "                       name=\"P5\",\n",
    "                       kernel_regularizer=regularizers.l2(0.0001))(P5)\n",
    "    P4 = layers.Conv2D(256,\n",
    "                       kernel_size=3,\n",
    "                       strides=1,\n",
    "                       padding='same',\n",
    "                       name=\"P4\",\n",
    "                       kernel_regularizer=regularizers.l2(0.0001))(P4)\n",
    "    P3 = layers.Conv2D(256,\n",
    "                       kernel_size=3,\n",
    "                       strides=1,\n",
    "                       padding='same',\n",
    "                       name=\"P3\",\n",
    "                       kernel_regularizer=regularizers.l2(0.0001))(P3)\n",
    "    # classification subnet\n",
    "    cls_subnet = _classification_sub_net(num_classes=num_classes, num_anchor=num_anchor)\n",
    "    P3_cls = cls_subnet(P3)\n",
    "    P4_cls = cls_subnet(P4)\n",
    "    P5_cls = cls_subnet(P5)\n",
    "    P6_cls = cls_subnet(P6)\n",
    "    P7_cls = cls_subnet(P7)\n",
    "    cls_output = layers.Concatenate(axis=-2)([P3_cls, P4_cls, P5_cls, P6_cls, P7_cls])\n",
    "    # localization subnet\n",
    "    loc_subnet = _regression_sub_net(num_anchor=num_anchor)\n",
    "    P3_loc = loc_subnet(P3)\n",
    "    P4_loc = loc_subnet(P4)\n",
    "    P5_loc = loc_subnet(P5)\n",
    "    P6_loc = loc_subnet(P6)\n",
    "    P7_loc = loc_subnet(P7)\n",
    "    loc_output = layers.Concatenate(axis=-2)([P3_loc, P4_loc, P5_loc, P6_loc, P7_loc])\n",
    "    return tf.keras.Model(inputs=inputs, outputs=[cls_output, loc_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaLoss(TensorOp):\n",
    "    def forward(self, data, state):\n",
    "        anchorbox, cls_pred, loc_pred = data\n",
    "        batch_size = anchorbox.shape[0]\n",
    "        focal_loss, l1_loss, total_loss = [], [], []\n",
    "        for idx in range(batch_size):\n",
    "            single_loc_gt, single_cls_gt = anchorbox[idx][:, :-1], tf.cast(anchorbox[idx][:, -1], tf.int32)\n",
    "            single_loc_pred, single_cls_pred = loc_pred[idx], cls_pred[idx]\n",
    "            single_focal_loss, anchor_obj_idx = self.focal_loss(single_cls_gt, single_cls_pred)\n",
    "            single_l1_loss = self.smooth_l1(single_loc_gt, single_loc_pred, anchor_obj_idx)\n",
    "            focal_loss.append(single_focal_loss)\n",
    "            l1_loss.append(single_l1_loss)\n",
    "        focal_loss, l1_loss = tf.reduce_mean(focal_loss), tf.reduce_mean(l1_loss)\n",
    "        total_loss = focal_loss + l1_loss\n",
    "        return total_loss, focal_loss, l1_loss\n",
    "\n",
    "    def focal_loss(self, single_cls_gt, single_cls_pred, alpha=0.25, gamma=2.0):\n",
    "        # single_cls_gt shape: [num_anchor], single_cls_pred shape: [num_anchor, num_class]\n",
    "        num_classes = single_cls_pred.shape[-1]\n",
    "        # gather the objects and background, discard the rest\n",
    "        anchor_obj_idx = tf.where(tf.greater_equal(single_cls_gt, 0))\n",
    "        anchor_obj_bg_idx = tf.where(tf.greater_equal(single_cls_gt, -1))\n",
    "        anchor_obj_count = tf.cast(tf.shape(anchor_obj_idx)[0], tf.float32)\n",
    "        single_cls_gt = tf.one_hot(single_cls_gt, num_classes)\n",
    "        single_cls_gt = tf.gather_nd(single_cls_gt, anchor_obj_bg_idx)\n",
    "        single_cls_pred = tf.gather_nd(single_cls_pred, anchor_obj_bg_idx)\n",
    "        single_cls_gt = tf.reshape(single_cls_gt, (-1, 1))\n",
    "        single_cls_pred = tf.reshape(single_cls_pred, (-1, 1))\n",
    "        # compute the focal weight on each selected anchor box\n",
    "        alpha_factor = tf.ones_like(single_cls_gt) * alpha\n",
    "        alpha_factor = tf.where(tf.equal(single_cls_gt, 1), alpha_factor, 1 - alpha_factor)\n",
    "        focal_weight = tf.where(tf.equal(single_cls_gt, 1), 1 - single_cls_pred, single_cls_pred)\n",
    "        focal_weight = alpha_factor * focal_weight**gamma / anchor_obj_count\n",
    "        cls_loss = tf.losses.BinaryCrossentropy(reduction='sum')(single_cls_gt,\n",
    "                                                                 single_cls_pred,\n",
    "                                                                 sample_weight=focal_weight)\n",
    "        return cls_loss, anchor_obj_idx\n",
    "\n",
    "    def smooth_l1(self, single_loc_gt, single_loc_pred, anchor_obj_idx, beta=0.1):\n",
    "        # single_loc_gt shape: [num_anchor x 4], anchor_obj_idx shape:  [num_anchor x 4]\n",
    "        single_loc_pred = tf.gather_nd(single_loc_pred, anchor_obj_idx)  #anchor_obj_count x 4\n",
    "        single_loc_gt = tf.gather_nd(single_loc_gt, anchor_obj_idx)  #anchor_obj_count x 4\n",
    "        anchor_obj_count = tf.cast(tf.shape(single_loc_pred)[0], tf.float32)\n",
    "        single_loc_gt = tf.reshape(single_loc_gt, (-1, 1))\n",
    "        single_loc_pred = tf.reshape(single_loc_pred, (-1, 1))\n",
    "        loc_diff = tf.abs(single_loc_gt - single_loc_pred)\n",
    "        cond = tf.less(loc_diff, beta)\n",
    "        loc_loss = tf.where(cond, 0.5 * loc_diff**2 / beta, loc_diff - 0.5 * beta)\n",
    "        loc_loss = tf.reduce_sum(loc_loss) / anchor_obj_count\n",
    "        return loc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_fn(step):\n",
    "    if step < 2000:\n",
    "        lr = (0.01 - 0.0002) / 2000 * step + 0.0002\n",
    "    elif step < 120000:\n",
    "        lr = 0.01\n",
    "    elif step < 160000:\n",
    "        lr = 0.001\n",
    "    else:\n",
    "        lr = 0.0001\n",
    "    return lr / 2  # original batch_size 16, for 512 we have batch_size 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "model = fe.build(model_fn=lambda: RetinaNet(input_shape=(512, 512, 3), num_classes=90),\n",
    "                 optimizer_fn=lambda: tf.optimizers.SGD(momentum=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictBox(TensorOp):\n",
    "    \"\"\"Convert network output to bounding boxes.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 inputs=None,\n",
    "                 outputs=None,\n",
    "                 mode=None,\n",
    "                 input_shape=(512, 512, 3),\n",
    "                 select_top_k=1000,\n",
    "                 nms_max_outputs=100,\n",
    "                 score_threshold=0.05):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.input_shape = input_shape\n",
    "        self.select_top_k = select_top_k\n",
    "        self.nms_max_outputs = nms_max_outputs\n",
    "        self.score_threshold = score_threshold\n",
    "\n",
    "        all_anchors, num_anchors_per_level = _get_fpn_anchor_box(width=input_shape[1], height=input_shape[0])\n",
    "        self.all_anchors = tf.convert_to_tensor(all_anchors)\n",
    "        self.num_anchors_per_level = num_anchors_per_level\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        pred = []\n",
    "\n",
    "        # extract max score and its class label\n",
    "        cls_pred, deltas, bbox = data\n",
    "        batch_size = bbox.shape[0]\n",
    "        labels = tf.cast(tf.argmax(cls_pred, axis=2), dtype=tf.int32)\n",
    "        scores = tf.reduce_max(cls_pred, axis=2)\n",
    "\n",
    "\n",
    "        # iterate over images\n",
    "        for i in range(batch_size):\n",
    "            # split batch into images\n",
    "            labels_per_image = labels[i]\n",
    "            scores_per_image = scores[i]\n",
    "            deltas_per_image = deltas[i]\n",
    "\n",
    "            selected_deltas_per_image = tf.constant([], shape=(0, 4))\n",
    "            selected_labels_per_image = tf.constant([], dtype=tf.int32)\n",
    "            selected_scores_per_image = tf.constant([])\n",
    "            selected_anchor_indices_per_image = tf.constant([], dtype=tf.int32)\n",
    "\n",
    "            end_index = 0\n",
    "            # iterate over each pyramid level\n",
    "            for j in range(self.num_anchors_per_level.shape[0]):\n",
    "                start_index = end_index\n",
    "                end_index += self.num_anchors_per_level[j]\n",
    "                anchor_indices = tf.range(start_index, end_index, dtype=tf.int32)\n",
    "\n",
    "                level_scores = scores_per_image[start_index:end_index]\n",
    "                level_deltas = deltas_per_image[start_index:end_index]\n",
    "                level_labels = labels_per_image[start_index:end_index]\n",
    "\n",
    "                # select top k\n",
    "                if self.num_anchors_per_level[j] >= self.select_top_k:\n",
    "                    top_k = tf.math.top_k(level_scores, self.select_top_k)\n",
    "                    top_k_indices = top_k.indices\n",
    "                else:\n",
    "                    top_k_indices = tf.subtract(anchor_indices, [start_index])\n",
    "\n",
    "                # combine all pyramid levels\n",
    "                selected_deltas_per_image = tf.concat(\n",
    "                    [selected_deltas_per_image, tf.gather(level_deltas, top_k_indices)], axis=0)\n",
    "                selected_scores_per_image = tf.concat(\n",
    "                    [selected_scores_per_image, tf.gather(level_scores, top_k_indices)], axis=0)\n",
    "                selected_labels_per_image = tf.concat(\n",
    "                    [selected_labels_per_image, tf.gather(level_labels, top_k_indices)], axis=0)\n",
    "                selected_anchor_indices_per_image = tf.concat(\n",
    "                    [selected_anchor_indices_per_image, tf.gather(anchor_indices, top_k_indices)], axis=0)\n",
    "\n",
    "            # delta -> (x1, y1, w, h)\n",
    "            selected_anchors_per_image = tf.gather(self.all_anchors, selected_anchor_indices_per_image)\n",
    "            x1 = (selected_deltas_per_image[:, 0] * selected_anchors_per_image[:, 2]) + selected_anchors_per_image[:, 0]\n",
    "            y1 = (selected_deltas_per_image[:, 1] * selected_anchors_per_image[:, 3]) + selected_anchors_per_image[:, 1]\n",
    "            w = tf.math.exp(selected_deltas_per_image[:, 2]) * selected_anchors_per_image[:, 2]\n",
    "            h = tf.math.exp(selected_deltas_per_image[:, 3]) * selected_anchors_per_image[:, 3]\n",
    "            x2 = x1 + w\n",
    "            y2 = y1 + h\n",
    "\n",
    "            # nms\n",
    "            # filter out low score, and perform nms\n",
    "            boxes_per_image = tf.stack([y1, x1, y2, x2], axis=1)\n",
    "            nms_indices = tf.image.non_max_suppression(boxes_per_image,\n",
    "                                                       selected_scores_per_image,\n",
    "                                                       self.nms_max_outputs,\n",
    "                                                       score_threshold=self.score_threshold)\n",
    "\n",
    "            nms_boxes = tf.gather(boxes_per_image, nms_indices)\n",
    "            final_scores = tf.gather(selected_scores_per_image, nms_indices)\n",
    "            final_labels = tf.cast(tf.gather(selected_labels_per_image, nms_indices), dtype=tf.float32)\n",
    "\n",
    "            # clip bounding boxes to image size\n",
    "            x1 = tf.clip_by_value(nms_boxes[:, 1], clip_value_min=0, clip_value_max=self.input_shape[1])\n",
    "            y1 = tf.clip_by_value(nms_boxes[:, 0], clip_value_min=0, clip_value_max=self.input_shape[0])\n",
    "            w = tf.clip_by_value(nms_boxes[:, 3], clip_value_min=0, clip_value_max=self.input_shape[1]) - x1\n",
    "            h = tf.clip_by_value(nms_boxes[:, 2], clip_value_min=0, clip_value_max=self.input_shape[0]) - y1\n",
    "\n",
    "            image_results = tf.stack([x1, y1, w, h, final_labels, final_scores], axis=1)\n",
    "            pred.append(image_results)\n",
    "            \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = fe.Network(ops=[\n",
    "    ModelOp(model=model, inputs=\"image\", outputs=[\"cls_pred\", \"loc_pred\"]),\n",
    "    RetinaLoss(inputs=[\"anchorbox\", \"cls_pred\", \"loc_pred\"], outputs=[\"total_loss\", \"focal_loss\", \"l1_loss\"]),\n",
    "    UpdateOp(model=model, loss_name=\"total_loss\"),\n",
    "    PredictBox(inputs=[\"cls_pred\", \"loc_pred\", \"bbox\"], outputs=\"pred\", mode=\"eval\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = fe.Estimator(\n",
    "    pipeline=pipeline,\n",
    "    network=network,\n",
    "    epochs=epochs,\n",
    "    max_steps_per_epoch=max_steps_per_epoch,\n",
    "    traces=[\n",
    "        LRScheduler(model=model, lr_fn=lr_fn),\n",
    "        BestModelSaver(model=model, save_dir='./', metric='mAP', save_best_mode=\"max\"),\n",
    "        MeanAveragePrecision(num_classes=90)\n",
    "    ],\n",
    "    monitor_names=[\"l1_loss\", \"focal_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "FastEstimator-Start: step: 1; model_lr: 0.01; \n",
      "INFO:tensorflow:batch_all_reduce: 248 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "FastEstimator-Train: step: 1; l1_loss: 1.1807466; focal_loss: 0.87915313; total_loss: 0.8791387; model_lr: 0.00010245; \n",
      "FastEstimator-Train: step: 2; epoch: 1; epoch_time: 20.61 sec; \n",
      "WARNING:tensorflow:5 out of the last 6 calls to functools.partial(<bound method MirroredExtended._call_for_each_replica of <tensorflow.python.distribute.mirrored_strategy.MirroredExtended object at 0x7f9da13ea410>>, <function TFNetwork._forward_step_static at 0x7f9d581e3440>) triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 7 calls to functools.partial(<bound method MirroredExtended._call_for_each_replica of <tensorflow.python.distribute.mirrored_strategy.MirroredExtended object at 0x7f9da13ea410>>, <function TFNetwork._forward_step_static at 0x7f9d581e3440>) triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 8 calls to functools.partial(<bound method MirroredExtended._call_for_each_replica of <tensorflow.python.distribute.mirrored_strategy.MirroredExtended object at 0x7f9da13ea410>>, <function TFNetwork._forward_step_static at 0x7f9d581e3440>) triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 9 calls to functools.partial(<bound method MirroredExtended._call_for_each_replica of <tensorflow.python.distribute.mirrored_strategy.MirroredExtended object at 0x7f9da13ea410>>, <function TFNetwork._forward_step_static at 0x7f9d581e3440>) triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "FastEstimator-ModelSaver: saved model to ./model_best_mAP.h5\n",
      "FastEstimator-Eval: step: 2; epoch: 1; total_loss: 1.4572886; l1_loss: 1.2779561; focal_loss: 0.94252574; min_total_loss: 1.4572886; since_best: 0; mAP: 0.0; AP50: 0.0; AP75: 0.0; \n",
      "FastEstimator-Train: step: 4; epoch: 2; epoch_time: 5.48 sec; \n",
      "FastEstimator-Eval: step: 4; epoch: 2; total_loss: 13.387227; l1_loss: 1.2782671; focal_loss: 0.9427077; min_total_loss: 1.4572886; since_best: 1; mAP: 0.0; AP50: 0.0; AP75: 0.0; \n",
      "FastEstimator-Train: step: 6; epoch: 3; epoch_time: 5.81 sec; \n",
      "FastEstimator-Eval: step: 6; epoch: 3; total_loss: 3.1068526e+18; l1_loss: 5.4488404e+18; focal_loss: 2.7244202e+18; min_total_loss: 1.4572886; since_best: 2; mAP: 0.0; AP50: 0.0; AP75: 0.0; \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-28be9b2c12b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/fastestimator/fastestimator/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, summary)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warmup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/fastestimator/fastestimator/estimator.py\u001b[0m in \u001b[0;36m_start_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"train\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"eval\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/fastestimator/fastestimator/estimator.py\u001b[0m in \u001b[0;36m_run_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mSuppressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/distribute/input_lib.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/distribute/input_lib.py\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    304\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m     \u001b[0mglobal_has_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_as_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_devices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/distribute/input_lib.py\u001b[0m in \u001b[0;36m_get_next_as_optional\u001b[0;34m(iterator, strategy, name)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m       worker_has_value, next_element = (\n\u001b[0;32m--> 202\u001b[0;31m           iterator._iterators[i].get_next_as_list(new_name))  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Collective all-reduce requires explict devices for inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/distribute/input_lib.py\u001b[0m in \u001b[0;36mget_next_as_list\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m       \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_as_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/data/ops/multi_device_iterator_ops.py\u001b[0m in \u001b[0;36mget_next_as_optional\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    338\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         result.append(\n\u001b[0;32m--> 340\u001b[0;31m             iterator_ops.get_next_as_optional(self._device_iterators[i]))\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mget_next_as_optional\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    826\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flat_tensor_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m           output_shapes=structure.get_flat_tensor_shapes(\n\u001b[0;32m--> 828\u001b[0;31m               iterator.element_spec)), iterator.element_spec)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/eager/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, message, code)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Exception class to handle not ok Status.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './'\n",
    "weights_path = os.path.join(save_dir, \"model1_best_total_loss_20200407.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fe.build(model_fn=lambda: RetinaNet(input_shape=(512, 512, 3), num_classes=90),\n",
    "                 optimizer_fn=None,\n",
    "                 model_names=\"retinanet\",\n",
    "                 weights_path=weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = fe.Network(ops=[\n",
    "    ModelOp(model=model, inputs=\"image\", outputs=[\"cls_pred\", \"loc_pred\"]),\n",
    "    #RetinaLoss(inputs=[\"anchorbox\", \"cls_pred\", \"loc_pred\"], outputs=[\"total_loss\", \"focal_loss\", \"l1_loss\"]),\n",
    "    PredictBox(inputs=[\"cls_pred\", \"loc_pred\", \"bbox\"],\n",
    "               outputs=\"pred\",\n",
    "               mode=\"infer\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = np.random.randint(len(eval_ds), size=batch_size).tolist()\n",
    "print(selected)\n",
    "data = [pipeline.transform(eval_ds[i], mode=\"infer\") for i in selected]\n",
    "im = np.array([item['image'][0] for item in data])\n",
    "pad = max(item['bbox'][0].shape[0] for item in data)\n",
    "bo = np.array([\n",
    "    np.pad(item['bbox'][0], ((0, pad - item['bbox'][0].shape[0]), (0, 0)), mode='constant', constant_values=0)\n",
    "    for item in data\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_out = network.transform({'image': im, 'bbox': bo}, mode=\"infer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #batch_index = 0\n",
    "# fig, ax = plt.subplots(batch_size, 2, figsize=(20, 60))\n",
    "# for batch_index in range(batch_size):\n",
    "#     img = network_out['image'].numpy()[batch_index, ...]\n",
    "#     img = ((img + 1) / 2 * 255).astype(np.uint8)\n",
    "\n",
    "#     img2 = img.copy()\n",
    "\n",
    "#     keep = network_out['bbox'][batch_index].numpy()[..., -1] > 0\n",
    "#     gt_x1, gt_y1, gt_w, gt_h, gt_label = network_out['bbox'][batch_index].numpy()[keep].T\n",
    "#     gt_x2 = gt_x1 + gt_w\n",
    "#     gt_y2 = gt_y1 + gt_h\n",
    "\n",
    "#     scores = network_out['pred'][batch_index].numpy()[..., -1]\n",
    "#     labels = network_out['pred'][batch_index].numpy()[..., -2]\n",
    "#     keep = scores > 0.5\n",
    "#     x1, y1, w, h, label, _ = network_out['pred'][batch_index].numpy()[keep].T\n",
    "#     x2 = x1 + w\n",
    "#     y2 = y1 + h\n",
    "\n",
    "#     for i in range(len(gt_x1)):\n",
    "#         cv2.rectangle(img, (gt_x1[i], gt_y1[i]), (gt_x2[i], gt_y2[i]), (255, 0, 0), 2)\n",
    "#         ax[batch_index, 0].set_xlabel('GT', fontsize=14, fontweight='bold')\n",
    "#         ax[batch_index, 0].text(gt_x1[i] + 3,\n",
    "#                    gt_y1[i] + 12,\n",
    "#                    class_map[str(int(gt_label[i]))],\n",
    "#                    color=(1, 0, 0),\n",
    "#                    fontsize=14,\n",
    "#                    fontweight='bold')\n",
    "\n",
    "#     for j in range(len(x1)):\n",
    "#         cv2.rectangle(img2, (x1[j], y1[j]), (x2[j], y2[j]), (100, 240, 240), 2)\n",
    "#         ax[batch_index, 1].set_xlabel('Prediction', fontsize=14, fontweight='bold')\n",
    "#         ax[batch_index, 1].text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]))], color=(0.5, 0.9, 0.9), fontsize=14, fontweight='bold')\n",
    "\n",
    "#     ax[batch_index, 0].imshow(img)\n",
    "#     ax[batch_index, 1].imshow(img2)\n",
    "# #     print(scores)\n",
    "# #     print(list(map(class_map.get, labels.astype(int).astype(str))))\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_out.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network_out['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = to_number(network_out['bbox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension is `(batch, padded_boxes, [x1, y1, w, h, label])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox[0][:, -1] != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(bbox):\n",
    "    assert np.array_equal(bbox[i], bbox.reshape(-1, 5)[bbox.shape[1]*i:bbox.shape[1]*(i+1), ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = np.expand_dims(np.repeat(range(bbox.shape[0]), bbox.shape[1], axis=None), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reshape_gt(array: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Reshape ground truth and add local image id within batch.\n",
    "\n",
    "    The ground truth array has shape (batch_size, num_bbox, 5). The 5 is [x1, y1, w, h, label] for each bounding\n",
    "    box. We suppress the padded bounding boxes, and flatten the batch dimwnsion. The output shape is\n",
    "    (batch_size * num_bbox, 6). The 6 is [id_in_batch, x1, y1, w, h, label].\n",
    "\n",
    "    Args:\n",
    "        array: Ground truth with shape (batch_size, num_bbox, 5).\n",
    "\n",
    "    Returns:\n",
    "        Ground truth with shape (batch_size * num_bbox, 6).\n",
    "    \"\"\"\n",
    "    local_ids = np.repeat(range(array.shape[0]), array.shape[1], axis=None)\n",
    "    local_ids = np.expand_dims(local_ids, axis=-1)\n",
    "\n",
    "    gt_with_id = np.concatenate([local_ids, array.reshape(-1, 5)], axis=1)\n",
    "    keep = gt_with_id[..., -1] > 0\n",
    "\n",
    "    return gt_with_id[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network_out['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(network_out['pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list containing `(batch_size)` elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(network_out['pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the list, the dimension is `(up_to_max_det, [x1, y1, w, h, label, score])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = list(map(to_number, network_out['pred']))\n",
    "[item.shape for item in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_with_id = []\n",
    "for index, item in enumerate(pred):\n",
    "    local_ids = np.repeat([index], item.shape[0], axis=None)\n",
    "    local_ids = np.expand_dims(local_ids, axis=-1)\n",
    "    pred_with_id.append(np.concatenate([local_ids, item], axis=1))\n",
    "    \n",
    "pred_with_id = np.concatenate(pred_with_id, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([np.array([[1,2], [5,7]]), np.array([[6,8],[3,4]])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_with_id[15:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0][1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reshape_pred(pred: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Reshape predicted bounding boxes and add local image id within batch.\n",
    "\n",
    "    The input prediction array is a list of batch_size elements. For each element, it \n",
    "    has shape (num_bbox, 6). The 6 is [x1, y1, w, h, label, score] for each bounding\n",
    "    box. For output we flatten the batch dimension. The output shape is\n",
    "    (batch_size * num_bbox, 6). The 6 is [id_in_batch, x1, y1, w, h, label].\n",
    "\n",
    "    Args:\n",
    "        array: Ground truth with shape (batch_size, num_bbox, 5).\n",
    "\n",
    "    Returns:\n",
    "        Ground truth with shape (batch_size * num_bbox, 6).\n",
    "    \"\"\"\n",
    "    pred_with_id = []\n",
    "    for index, item in enumerate(pred):\n",
    "        local_ids = np.repeat([index], item.shape[0], axis=None)\n",
    "        local_ids = np.expand_dims(local_ids, axis=-1)\n",
    "        pred_with_id.append(np.concatenate([local_ids, item], axis=1))\n",
    "    \n",
    "    pred_with_id = np.concatenate(pred_with_id, axis=0)\n",
    "    return pred_with_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = defaultdict(list)\n",
    "test['a', 'b'].append([30, 40])\n",
    "test['c', 'd'].append(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(test):\n",
    "    print(i)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['a', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = {}\n",
    "test2['c', 'd'] = {'x': 5, 'y': 4}\n",
    "test2['a', 'b'] = {'x': 2, 'y': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
